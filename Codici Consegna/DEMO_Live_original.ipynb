{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://datascience.disco.unimib.it/it/\" target=\"_parent\">\n",
    "<img src=\"https://raw.githubusercontent.com/malborroni/Foundations_of_Computer-Science/master/images/DSunimib.png\" alt=\"Open the site\" width=95% align='left'/></a>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/malborroni/RECMojion/blob/master/images/miscellaneous/Logo%206.png\" target=\"_parent\">\n",
    "<img src=\"https://raw.githubusercontent.com/malborroni/RECMojion/master/images/miscellaneous/Logo%206.png\" alt=\"Logo\" width=35% align='center'/></a>\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=894E97> Live Cam Detection with Classification </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "> - __Authors__: &nbsp;&nbsp;Borroni Alessandro, Corvaglia Andre, Perletti Massimiliano\n",
    "- __ID numbers__: &nbsp;&nbsp;800069, 802487, 847548\n",
    "- __Academic year__: &nbsp;&nbsp;2019/2020\n",
    "- __CdLM__: &nbsp;&nbsp;Data Science\n",
    "- __E-mails__: &nbsp;&nbsp;a.borroni2@campus.unimib.it, a.corvaglia3@campus.unimib.it, m.perletti2@campus.unimib.it\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=009DC6> 1. Import packages </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done it yet, install this repository from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2 as cv\n",
    "from cv2 import VideoCapture as cap\n",
    "from skimage import io\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "from utils import CFEVideoConf, image_resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=009DC6> 2. Load the model and define the functions: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MioPC\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models are succesfully loaded\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('weights-senet50-full2.best.hdf5')\n",
    "#print('Loaded model from disk')\n",
    "\n",
    "mod1 = load_model('preproc_final.h5')\n",
    "#mod2 = load_model('pre_optimization.h5')\n",
    "mod2 = load_model('opt_best.h5')\n",
    "print('Both models are succesfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(img, model):\n",
    "    #-- fase di preprocess per l'immagine (to vggface input)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    x = img.astype('float32')\n",
    "    x = preprocess_input(x, version = 2)\n",
    "    \n",
    "    #-- predict sul modello pre-trainato\n",
    "    predicted = mod1.predict(x)\n",
    "    #tmp = mod1.predict(x)\n",
    "    predicted = model.predict(predicted)\n",
    "    \n",
    "    #if np.max(predicted) >= .5:\n",
    "    #-- acquiszione valore di accuracy\n",
    "    acc = round(np.max(predicted), 3)\n",
    "        \n",
    "    #-- selezione per il labelling\n",
    "    pred = np.argmax(predicted, axis = 1)\n",
    "    if pred == 0:\n",
    "        label = 'Angry'\n",
    "        emoji = cv.imread(\"images/emoji/angry_mod.png\", -1)\n",
    "    if pred == 1:\n",
    "        label = 'Disgust'\n",
    "        emoji = cv.imread(\"images/emoji/disgust_mod.png\", -1)\n",
    "    if pred == 2:\n",
    "        label = 'Fear'\n",
    "        emoji = cv.imread(\"images/emoji/fear_mod.png\", -1)\n",
    "    if pred == 3:\n",
    "        label = 'Happy'\n",
    "        emoji = cv.imread(\"images/emoji/happy_mod.png\", -1)\n",
    "    if pred == 4:\n",
    "        label = 'Neutral'\n",
    "        emoji = cv.imread(\"images/emoji/neutral_mod.png\", -1)\n",
    "    if pred == 5:\n",
    "        label = 'Sad'\n",
    "        emoji = cv.imread(\"images/emoji/sad_mod.png\", -1)\n",
    "    if pred == 6:\n",
    "        label = 'Surprise'\n",
    "        emoji = cv.imread(\"images/emoji/surprise_mod.png\", -1)\n",
    "#     else:\n",
    "#         label = 'Unknown'\n",
    "\n",
    "    return label, acc, emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image, model, classifier,\n",
    "           scale_factor = 1.1, minNeighbors = 5,\n",
    "           size = (224,224)):\n",
    "    '''\n",
    "    This function takes in input images and after face detection apply a crop and a resize to that images.\n",
    "    \n",
    "    @params:\n",
    "    from_directory = where to find the photos\n",
    "    to_directory = where to save the photos\n",
    "    classifier = face detector pre-trained (XML file)\n",
    "    size = tuple\n",
    "    color = RGB or BW\n",
    "    init_number = first number to start naming photos\n",
    "    '''\n",
    "    face_cascade = cv.CascadeClassifier(classifier)\n",
    "    gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scale_factor, minNeighbors)\n",
    "\n",
    "    if len(faces) == 1:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+h] # rec\n",
    "            roi_color = image[y:y+h, x:x+w]\n",
    "            n_img = cv.resize(roi_color, size)\n",
    "        \n",
    "            #-- color B, G, R\n",
    "            \n",
    "            #color_face = (80,130,0)\n",
    "            #rectangle_bgr = (80,130,0)\n",
    "            \n",
    "            #color_face = (24,191,255) #oro\n",
    "            color_text = (255,255,255) #bianco\n",
    "            #rectangle_bgr = (24,191,255) #oro\n",
    "            #color_face = (255,204,0) #azzurro\n",
    "            #color_text = (0,153,255) #arancione\n",
    "            #rectangle_bgr = (255,204,0) #arancione\n",
    "            \n",
    "            #-- setup font e size per il testo\n",
    "            font = cv.FONT_HERSHEY_TRIPLEX\n",
    "            #font = cv.FONT_HERSHEY_DUPLEX\n",
    "            font_scale = 0.88\n",
    "            \n",
    "            #-- rettangolo face detection\n",
    "            #cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            #QUI\n",
    "            \n",
    "            #-- labelling (con model)\n",
    "            label, acc, emoji = labelling(n_img, model)\n",
    "            #-- preparo la stringa da stampare\n",
    "            string = \" \" + label + \": \" + str(acc) + \" \"\n",
    "            \n",
    "            if label == 'Angry':\n",
    "                color_face = (0,32,225)\n",
    "                rectangle_bgr = (0,32,225)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Disgust':\n",
    "                color_face = (0,66,96)\n",
    "                rectangle_bgr = (0,66,96)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Fear':\n",
    "                color_face = (95,0,64)\n",
    "                rectangle_bgr = (95,0,64)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Happy':\n",
    "                color_face = (0,128,0)\n",
    "                rectangle_bgr = (0,128,0)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Neutral':\n",
    "                color_face = (94,96,55)\n",
    "                rectangle_bgr = (94,96,55)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Sad':\n",
    "                color_face = (128,47,0)\n",
    "                rectangle_bgr = (128,47,0)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            if label == 'Surprise':\n",
    "                color_face = (24,191,255)\n",
    "                rectangle_bgr = (24,191,255)\n",
    "                cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            \n",
    "            #-- rettangolo per background stringa\n",
    "            (text_width, text_height) = cv.getTextSize(string, font, fontScale=font_scale, thickness=1)[0]\n",
    "            box_coords = ((x-2, y), (x + text_width + 4, y - text_height - 8))\n",
    "            cv.rectangle(image, box_coords[0], box_coords[1], rectangle_bgr, cv.FILLED)\n",
    "            #-- inserimento stringa\n",
    "            cv.putText(image, string, (x,y-5), font, font_scale, color_text, 1)\n",
    "            \n",
    "            for (ex, ey, ew, eh) in faces:\n",
    "                roi_face = roi_gray[ey: ey + eh, ex: ex + ew]\n",
    "                emojis = image_resize(emoji.copy(), width = w//4)\n",
    "                \n",
    "                #-- Salvo valori shape di \"faces\" \n",
    "                faces_h, faces_w = faces.shape\n",
    "                \n",
    "                gw, gh, gc = emojis.shape\n",
    "                \n",
    "                for i in range(0, gw):\n",
    "                    for j in range(0, gh):\n",
    "                        #print(emoji[i, j]) #RGBA\n",
    "                        if emojis[i, j][3] != 0: # alpha 0\n",
    "                            offset = 10\n",
    "                            h_offset = faces_h - gh - offset\n",
    "                            w_offset = faces_w - gw - offset\n",
    "                            roi_color[h_offset + i, w_offset + j] = emojis[i, j, :-1]\n",
    "            \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_cam(model, classifier, resolution = (1280, 720), fps = 30, time_sleep = 2):\n",
    "    '''\n",
    "    This function allow to access to pc webcam in order to acquire images of the subject.\n",
    "    \n",
    "    @params:\n",
    "    directory = where to save the captured images\n",
    "    subject = name of the person who is in photo\n",
    "    init_number = first number to start naming photos\n",
    "    counter_limit = if desired, limit the number of photos captured\n",
    "    resolution = (width, height)\n",
    "    fps = Frame Per Second\n",
    "    time_sleep = sleep between two photos\n",
    "    numbered = Bool, necessary in order to apply counter_limit\n",
    "    '''\n",
    "    cam = cv.VideoCapture(0) #maybe change here\n",
    "\n",
    "    cam.set(cv.CAP_PROP_FRAME_WIDTH, resolution[0])\n",
    "    cam.set(cv.CAP_PROP_FRAME_HEIGHT, resolution[1])\n",
    "    cam.set(cv.CAP_PROP_FPS, fps)\n",
    "\n",
    "    cv.namedWindow(\"Acquisition window\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        k = cv.waitKey(1)\n",
    "\n",
    "        time.sleep(time_sleep)\n",
    "\n",
    "        detect(frame, model, classifier=classifier, scale_factor = 1.1, minNeighbors = 6)\n",
    "        \n",
    "        text = 'Press ESC to exit from demo...'\n",
    "        cv.putText(frame, text, (10,20), cv.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 1)\n",
    "        \n",
    "        cv.imshow(\"Acquisition window\", frame)\n",
    "\n",
    "        \n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    cam.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=009DC6> 3. Launch the Live Demo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interrupt the video recording, press \"ESC\" on the keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "live_cam(time_sleep = 0.01, model = mod2, classifier = 'haarcascade_frontalface_alt2.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
